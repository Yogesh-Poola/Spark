{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca31731-6e19-4565-85cc-d337e78b961f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6291763126987047#setting/sparkui/0330-041602-nyl8iyiy/driver-2873487115772032971\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6291763126987047#setting/sparkui/0330-041602-nyl8iyiy/driver-2873487115772032971\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2122fb-c3fd-4769-89a8-2ee7c0d84efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/creationsbyyogesh@gmail.com/2010_12_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dde85ef-a2c0-42d4-80f5-b03d42bcd60c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520b8fbe-50c9-40fb-89d0-bf87104c2784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cafdced-7ac2-43d5-a5ec-84fe97bf7fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"df1Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39de708c-7a47-47b4-9139-8b241a8c05b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initially Data will be in their Native Datatypes. We need to convert them into Spark Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f482d742-afde-4987-afb2-fb7f2ece9360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|  5|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|  5|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  5|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|  5|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  5|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|  5|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+---+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df1.select(\"*\",lit(5)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163febc0-e7c2-4836-bf17-fbf7b556598a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|  5|\n+---+\n|  5|\n|  5|\n|  5|\n|  5|\n|  5|\n+---+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df1.select(lit(5)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67d77c7c-4dbf-43fd-8acf-966acd33961b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458a340a-2df4-422a-bdf4-1d81c0a29fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.where(col(\"InvoiceNo\")!='536365')\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed72607-df99-4318-9246-3c90c982db83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n|         Description|       Country|\n+--------------------+--------------+\n|HAND WARMER UNION...|United Kingdom|\n|HAND WARMER RED P...|United Kingdom|\n|ASSORTED COLOUR B...|United Kingdom|\n|POPPY'S PLAYHOUSE...|United Kingdom|\n|POPPY'S PLAYHOUSE...|United Kingdom|\n+--------------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.where(col(\"InvoiceNo\")!='536365')\\\n",
    "    .select(\"Description\", \"Country\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0313a451-95fc-4b2e-9d5b-f627e417d6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.where(\"InvoiceNo<>'536365'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35399ab-db22-4223-b413-a64377ba859a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write the following code using pyspark functions\n",
    "\n",
    "SELECT * FROM dfTable WHERE StockCode in (\"DOT\") AND(UnitPrice > 600 OR\n",
    "instr(Description, \"POSTAGE\") >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a8f3a1-0f4d-4368-9571-919d5211dfbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "df1.where(col(\"StockCode\").isin(\"DOT\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe08c905-86a7-4aac-9bb0-ceb2602a521f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark AND operator for multiple conditions - &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de99db40-719b-4e55-bf9d-93b108a86369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.where(col(\"StockCode\").isin(\"DOT\"))\\\n",
    "    .where((col(\"UnitPrice\")>600) & (instr(col(\"Description\"),\"POSTAGE\")>1)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2fe8592-7cec-4c15-b75f-a7ac68247742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark OR operator for multiple conditions - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b831fac-289d-4fc6-978c-3da161ffed32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = df1.UnitPrice > 600\n",
    "descriptionFilter = instr(df1.Description,\"POSTAGE\")>=1\n",
    "df1.where(col(\"StockCode\").isin(\"DOT\"))\\\n",
    "    .where(priceFilter | descriptionFilter).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d00ecb-0200-46ac-af15-bf70cb3c0789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78cf7ff7-cd4d-4beb-9103-910a691e3f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr,pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314f7f95-668f-4ac2-bfbc-e55c3ac5c51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fabricatedColumn = pow(col(\"Quantity\") * col(\"UnitPrice\"),2) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb50c22c-e650-4dae-9236-6eebaae03771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|CustomerID|         TruePrice|\n+----------+------------------+\n|   17850.0|239.08999999999997|\n|   17850.0|          418.7156|\n|   17850.0|             489.0|\n|   17850.0|          418.7156|\n|   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(col(\"CustomerID\"),fabricatedColumn.alias(\"TruePrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18df9bc0-be7b-4ee3-b2d6-91f036722b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|round(2.5, 0)|bround(2.5, 0)|\n+-------------+--------------+\n|          3.0|           2.0|\n|          3.0|           2.0|\n+-------------+--------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "df1.select(round(lit(2.5)), bround(lit(2.5))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed25840-453a-4cbc-8f18-781429dfdb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e940953-be6c-4c28-a663-67071fd489c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235c557f-4398-46df-b2cf-10778355c9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, initcap, ltrim, rtrim, trim, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d47f1d-51ab-44f7-b21c-2c324c34cc03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|initcap(Description)|\n+--------------------+\n|White Hanging Hea...|\n| White Metal Lantern|\n|Cream Cupid Heart...|\n|Knitted Union Fla...|\n|Red Woolly Hottie...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(initcap(col(\"Description\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133f8847-3421-4053-8785-673807d815c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "lpad(col, len, pad)\n",
    "\n",
    "rpad(col,len,pad)\tlpad() – Add a specified character as padding on the left side.\n",
    "\n",
    "rpad() – Add a specified character as padding on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a50fa5-9fa2-466d-bdd4-7ca7ba9b096e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------+----+----------+\n|         LTRIM|         RTRIM|       TRIM|LPAD|      RPAD|\n+--------------+--------------+-----------+----+----------+\n|Hello Madam   |   Hello Madam|Hello Madam|    |   Hello M|\n|Hello Madam   |   Hello Madam|Hello Madam|    |   Hello M|\n+--------------+--------------+-----------+----+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "string1=\"   Hello Madam   \"\n",
    "df1.select(ltrim(lit(string1)).alias(\"LTRIM\")\\\n",
    "    , rtrim(lit(string1)).alias(\"RTRIM\")\\\n",
    "    , trim(lit(string1)).alias(\"TRIM\")\\\n",
    "    , lpad(lit(string1),3,\" \").alias(\"LPAD\")\\\n",
    "    , rpad(lit(string1),10,\" \").alias(\"RPAD\")\n",
    "           ).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a75e2e-ff18-4ba8-af32-b3b81950c470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cdf1d8-ad3f-4898-be9b-3135a391b5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47efba25-b665-49e5-8123-07894996c3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|            Replaced|         Description|\n+--------------------+--------------------+\n|WHI73 H4NGING H34...|WHITE HANGING HEA...|\n| WHI73 M3741 14N73RN| WHITE METAL LANTERN|\n|CR34M CUPID H34R7...|CREAM CUPID HEART...|\n|KNI773D UNION F14...|KNITTED UNION FLA...|\n|R3D WOO11Y HO77I3...|RED WOOLLY HOTTIE...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(translate(col(\"Description\"),\"LEAT\",\"1347\").alias(\"Replaced\"),col(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed685e1b-2cd3-46f9-9247-22786e02dd47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Date & Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1aa6b3-a6aa-4024-ae4b-6b0ad18e298a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635c46d0-9f74-4243-a8b0-1963bdb21f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b2b599-088b-4da7-900d-c20aa3eb5eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfDate = spark.range(10)\\\n",
    "    .withColumn(\"Today Date\",current_date())\\\n",
    "    .withColumn(\"Time Now\",current_timestamp())\n",
    "dfDate.createOrReplaceTempView(\"dfDateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dca61d6-2a29-42ea-ba6a-8ce12adb2a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n| id|Today Date|            Time Now|\n+---+----------+--------------------+\n|  0|2025-03-29|2025-03-29 08:30:...|\n|  1|2025-03-29|2025-03-29 08:30:...|\n|  2|2025-03-29|2025-03-29 08:30:...|\n|  3|2025-03-29|2025-03-29 08:30:...|\n|  4|2025-03-29|2025-03-29 08:30:...|\n|  5|2025-03-29|2025-03-29 08:30:...|\n|  6|2025-03-29|2025-03-29 08:30:...|\n|  7|2025-03-29|2025-03-29 08:30:...|\n|  8|2025-03-29|2025-03-29 08:30:...|\n|  9|2025-03-29|2025-03-29 08:30:...|\n+---+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "dfDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacebc1a-8de1-4a59-9cfe-5516b87f4ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = false)\n |-- Today Date: date (nullable = false)\n |-- Time Now: timestamp (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "dfDate.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01de913c-ce2c-4171-9aaf-231cb728a22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_date_add_**\n",
    "\n",
    "**_date_sub_**\n",
    "\n",
    "**_datediff_**\n",
    "\n",
    "**_months_between_**\n",
    "\n",
    "**_to_date_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af19c6d-8bab-446b-9429-fafe1847d9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_sub, date_add, datediff, months_between, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5966265b-976e-42ee-9a56-6a8edb7a089b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n| id|Today Date|            Time Now|\n+---+----------+--------------------+\n|  0|2025-03-29|2025-03-29 08:39:...|\n|  1|2025-03-29|2025-03-29 08:39:...|\n|  2|2025-03-29|2025-03-29 08:39:...|\n|  3|2025-03-29|2025-03-29 08:39:...|\n|  4|2025-03-29|2025-03-29 08:39:...|\n|  5|2025-03-29|2025-03-29 08:39:...|\n|  6|2025-03-29|2025-03-29 08:39:...|\n|  7|2025-03-29|2025-03-29 08:39:...|\n|  8|2025-03-29|2025-03-29 08:39:...|\n|  9|2025-03-29|2025-03-29 08:39:...|\n+---+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "dfDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d88bf39-49ed-4cc1-9098-4b5b680a713d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+------------+------------+\n| id|Today Date|            Time Now|5 Days Prior|5 Days Later|\n+---+----------+--------------------+------------+------------+\n|  0|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  1|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  2|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  3|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  4|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  5|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  6|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  7|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  8|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n|  9|2025-03-29|2025-03-29 08:41:...|  2025-03-24|  2025-04-03|\n+---+----------+--------------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "dfDate.select(\"*\",date_sub(col(\"Today Date\"),5).alias(\"5 Days Prior\"),date_add(col(\"Today Date\"),5).alias(\"5 Days Later\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5495a759-e812-48cc-876e-0e5c8c0fc175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Difference between two dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d54f67-6ffe-481e-b415-417055585de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|Today Date|Difference in Days|\n+----------+------------------+\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n|2025-03-29|               -61|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "randomDate = to_date(lit('2025-01-27'))\n",
    "dfDate.select(col(\"Today Date\"),datediff(randomDate,col(\"Today Date\")).alias(\"Difference in Days\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea1a2e7-bac2-4c04-a68a-1ec605b952c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Months Between two dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bdf24c-51a0-4f93-bfd4-737013ea35be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n|months_between(to_date(2024-09-12), to_date(2025-02-28), true)|\n+--------------------------------------------------------------+\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n|                                                   -5.51612903|\n+--------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "startDate = to_date(lit('2024-09-12'))\n",
    "endDate = to_date(lit('2025-02-28'))\n",
    "dfDate.select(months_between(startDate, endDate)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de913888-d48d-44bb-a1ce-4d4626352d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f002588-bbb9-4372-95ed-5e0a7a2d172d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n|to_date(2016-20-12)|to_date(2017-12-11)|\n+-------------------+-------------------+\n|               null|         2017-12-11|\n+-------------------+-------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "dfDate.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2018ddff-a765-41a6-97d6-ce0520d8b279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**_We find this to be an especially tricky situation for bugs because some dates might match the\n",
    "correct format, whereas others do not. In the previous example, notice how the second date\n",
    "appears as Decembers 11th instead of the correct day, November 12th. Spark doesn’t throw an\n",
    "error because it cannot know whether the days are mixed up or that specific row is incorrect.\n",
    "Let’s fix this pipeline, step by step, and come up with a robust way to avoid these issues entirely.\n",
    "The first step is to remember that we need to specify our date format according to the Java\n",
    "SimpleDateFormat standard.\n",
    "We will use two functions to fix this: to_date and to_timestamp. The former optionally\n",
    "expects a format, whereas the latter requires one:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4ca67a-d15c-4a30-a9ca-516f0d05ddf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|     date1|     date2|\n+----------+----------+\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n|2016-12-20|2017-11-12|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "dateFormat ='yyyy-dd-MM'\n",
    "cleanedDfDate = dfDate.select(to_date(lit(\"2016-20-12\"),dateFormat).alias(\"date1\")\\\n",
    "    , to_date(lit(\"2017-12-11\"),dateFormat).alias(\"date2\"))\n",
    "cleanedDfDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f8e7da-804f-4116-8833-116edc4af2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n|to_timestamp(date1, yyyy-dd-MM)|\n+-------------------------------+\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n|            2016-12-20 00:00:00|\n+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanedDfDate.select(to_timestamp(col(\"date1\"),dateFormat)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109c7cad-b97f-4a27-b808-cc01ec632dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ab2daf-757c-4fe3-9363-f085046e871c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When we declare a column as not having a null time, that is not actually enforced. To reiterate, when\n",
    "you define a schema in which all columns are declared to not have null values, Spark will not enforce\n",
    "that and will happily let null values into that column. The nullable signal is simply to help Spark SQL\n",
    "optimize for handling that column. If you have null values in columns that should not have null values,\n",
    "you can get an incorrect result or see strange exceptions that can be difficult to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "198c37f1-7bd4-4e3a-b990-b8f084c58151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are two things you can do with null values: you can explicitly drop nulls or you can fill\n",
    "them with a value (globally or on a per-column basis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2dddcc-59b0-4c98-83bd-f570af9e9d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The simplest function is drop, which removes rows that contain nulls. The default is to drop any\n",
    "row in which any value is null:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6ed13c-011a-4b94-8687-807f7262d4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194477a9-fdf9-45b9-85ee-cfe75bf49f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
     ]
    }
   ],
   "source": [
    "df1.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5998642-dd53-4adb-b961-5a47f46d17c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Specifying \"any\" as an argument drops a row if any of the values are null. Using “all” drops the\n",
    "row only if all values are null or NaN for that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba642a3-8af4-4d75-9c02-dffe0dc4d778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
     ]
    }
   ],
   "source": [
    "df1.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8503db5c-4e7b-414b-8eb7-5ed315f7371e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can also apply this to certain sets of columns by passing in an array of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de26c85-806b-4203-a75b-b0684b2e47b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
     ]
    }
   ],
   "source": [
    "df1.na.drop(\"all\",subset=[\"StockCode\",\"Quantity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60617f95-90d9-4a3e-b798-402d1c6d67ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**fill**\n",
    "\n",
    "**Using the fill function, you can fill one or more columns with a set of values. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5430235c-6cc1-4498-89d3-0a2fe946feaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
     ]
    }
   ],
   "source": [
    "df1.na.fill(\"All NULL values become this string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cec837f-cccc-47c7-8c82-2efa25df9daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For multiple columns and values this can be done by specifying a map—that is a particular value and a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b08bb20-40a8-4ef3-9a9f-25fe4356b110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
     ]
    }
   ],
   "source": [
    "fill_na_cols = {\"StockCode\" : 5, \"Description\" : \"No Value\"}\n",
    "df1.na.fill(fill_na_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41a2c75e-84fe-49c2-9abc-b2ddd5a39e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Complex Types\n",
    "There are three kinds of complex types: **structs, arrays,\n",
    "and maps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0a89df-a307-409a-a5be-8949ba9bcff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Structs**\n",
    "\n",
    "You can think of structs as DataFrames within DataFrames. A worked example will illustrate\n",
    "this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1cea02-d4e2-4141-a2c1-a7ccf8fe0f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDf = df1.select(struct(\"StockCode\",\"Description\").alias(\"complex\"))\n",
    "complexDf.createOrReplaceTempView(\"complexDfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccef9911-ef1f-42d9-92a1-ff75f01295b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|             complex|\n+--------------------+\n|{85123A, WHITE HA...|\n|{71053, WHITE MET...|\n|{84406B, CREAM CU...|\n|{84029G, KNITTED ...|\n|{84029E, RED WOOL...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "complexDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d6f2a9-5d46-4a48-9bff-34e6338d10dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|complex.StockCode|\n+-----------------+\n|           85123A|\n|            71053|\n|           84406B|\n|           84029G|\n|           84029E|\n+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "complexDf.select(col(\"complex\").getField(\"StockCode\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4532ee1-b5ee-4f69-85ea-42a3dc5215d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Arrays**\n",
    "\n",
    "To define arrays, let’s work through a use case. With our current data, our objective is to take\n",
    "every single word in our Description column and convert that into a row in our DataFrame.\n",
    "The first task is to turn our Description column into a complex type, an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9363361a-886b-434f-9dbc-cacd6afaa1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**split**\n",
    "\n",
    "We do this by using the split function and specify the delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee12c83-bb1e-4c6a-968c-67690f63e155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|split(Description,  , -1)|\n+-------------------------+\n|     [WHITE, HANGING, ...|\n|     [WHITE, METAL, LA...|\n+-------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df1.select(split(col(\"Description\"),\" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ce71e8-f153-4db2-95d1-ae343726bb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|arrayDesc[0]|\n+------------+\n|       WHITE|\n|       WHITE|\n+------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(split(col(\"Description\"),\" \").alias(\"arrayDesc\"))\\\n",
    "    .selectExpr(\"arrayDesc[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8de5061-6636-4811-8d6c-2aa02c833c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**ArrayLength & ArrayContains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efaaf8e7-fec7-4c81-b695-cbbb7f6037e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9dfb8b-4833-4ffc-9e1f-46c1a30c3cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|arraySize|\n+---------+\n|        5|\n|        3|\n+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(size(split(col(\"Description\"),\" \")).alias(\"arraySize\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314e1ea2-22fe-4f30-8335-7300fcdc61c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|containsWhite|\n+-------------+\n|         true|\n|         true|\n+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\").alias(\"containsWhite\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c712f8d-eeb2-43bf-9d5f-516d921c837b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a312ba-6da7-47a4-aead-0eac735f2e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f53f28-5abf-4154-954e-d1e076d3bce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+\n|InvoiceNo|            splitted|exploded|\n+---------+--------------------+--------+\n|   536365|[WHITE, HANGING, ...|   WHITE|\n|   536365|[WHITE, HANGING, ...| HANGING|\n|   536365|[WHITE, HANGING, ...|   HEART|\n|   536365|[WHITE, HANGING, ...| T-LIGHT|\n|   536365|[WHITE, HANGING, ...|  HOLDER|\n|   536365|[WHITE, METAL, LA...|   WHITE|\n|   536365|[WHITE, METAL, LA...|   METAL|\n|   536365|[WHITE, METAL, LA...| LANTERN|\n|   536365|[CREAM, CUPID, HE...|   CREAM|\n|   536365|[CREAM, CUPID, HE...|   CUPID|\n+---------+--------------------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df1.withColumn(\"splitted\",split(\"Description\",\" \"))\\\n",
    "    .withColumn(\"exploded\",explode(\"splitted\"))\\\n",
    "    .select(col(\"InvoiceNo\"),col(\"splitted\"),col(\"exploded\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c4d513-827a-4af9-b6a7-0d737e6c18a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+--------------------+--------+\n|InvoiceNo|         Description|arraySize|            splitted|exploded|\n+---------+--------------------+---------+--------------------+--------+\n|   536365|WHITE HANGING HEA...|        5|[WHITE, HANGING, ...|   WHITE|\n|   536365|WHITE HANGING HEA...|        5|[WHITE, HANGING, ...| HANGING|\n|   536365|WHITE HANGING HEA...|        5|[WHITE, HANGING, ...|   HEART|\n|   536365|WHITE HANGING HEA...|        5|[WHITE, HANGING, ...| T-LIGHT|\n|   536365|WHITE HANGING HEA...|        5|[WHITE, HANGING, ...|  HOLDER|\n|   536365| WHITE METAL LANTERN|        3|[WHITE, METAL, LA...|   WHITE|\n|   536365| WHITE METAL LANTERN|        3|[WHITE, METAL, LA...|   METAL|\n|   536365| WHITE METAL LANTERN|        3|[WHITE, METAL, LA...| LANTERN|\n|   536365|CREAM CUPID HEART...|        5|[CREAM, CUPID, HE...|   CREAM|\n|   536365|CREAM CUPID HEART...|        5|[CREAM, CUPID, HE...|   CUPID|\n+---------+--------------------+---------+--------------------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"splitted\",split(\"Description\",\" \"))\\\n",
    "    .withColumn(\"exploded\",explode(\"splitted\"))\\\n",
    "    .withColumn(\"arraySize\",size(col(\"splitted\")))\\\n",
    "    .select(col(\"InvoiceNo\"),col(\"Description\"),col(\"arraySize\"),col(\"splitted\"),col(\"exploded\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd70973-9921-49d4-ba35-96f191517724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e7e5ba-456a-4b7d-82a7-d389ab9d42c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Maps are created by using the map function and key-value pairs of columns. You then can select\n",
    "them just like you might select from an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0065445a-b227-4ee1-8f4f-2744bc8b2f2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n|complexMap[WHITE METAL LANTERN]|\n+-------------------------------+\n|                           null|\n|                         536365|\n|                           null|\n|                           null|\n|                           null|\n+-------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df1.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complexMap\"))\\\n",
    "    .selectExpr(\"complexMap['WHITE METAL LANTERN']\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377f10da-ed4e-464e-bf58-d0ce3b32f30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can also explode map types, which will turn them into columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb26708b-f9f1-483e-8375-98d27091f840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n|                 key| value|\n+--------------------+------+\n|WHITE HANGING HEA...|536365|\n| WHITE METAL LANTERN|536365|\n|CREAM CUPID HEART...|536365|\n|KNITTED UNION FLA...|536365|\n|RED WOOLLY HOTTIE...|536365|\n+--------------------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complexMap\"))\\\n",
    "  .select(explode(col(\"complexMap\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d2733b-9f19-4941-89b9-668004c5dd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804cc87b-05f8-4410-89d2-35191c19ab65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark has some unique support for working with JSON data. You can operate directly on strings of JSON in Spark and parse from JSON or extract JSON objects. Let’s begin by creating a JSON column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6792e898-2f8f-46b5-aaaf-90d356a92c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jsonDf = spark.range(1).selectExpr(\"\"\" '{\"myJsonKey\" : {\"myJsonValue\" : [1,2,3]}}' as jsonString \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715a8125-0980-4ae1-95c4-756b7cdc3ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|column|\n+------+\n|     2|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDf.select(get_json_object(col(\"jsonString\"),\"$.myJsonKey.myJsonValue[1]\").alias(\"column\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d14d951-60f2-49be-8662-09a7e9e3877c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|                  c0|\n+--------------------+\n|{\"myJsonValue\":[1...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "jsonDf.select(json_tuple(col(\"jsonString\"),\"myJsonKey\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f46a69-4292-492d-8cfe-4d0ce840bab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Different Types of Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}