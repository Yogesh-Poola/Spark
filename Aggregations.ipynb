{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81573d34-84fa-4b15-9bf1-ea6439750366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d233b711-7515-4eed-a43f-fe6ac789c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Aggregations_Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3a0f89-94cc-4b30-b7a1-3d2256061c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "_options = {\n",
    "    \"header\" : \"true\",\n",
    "    \"inferSchema\" : \"true\",\n",
    "}\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .options(**_options)\\\n",
    "    .load('/Users/yogeshpoola/Downloads/DataVidhya Data/apache-spark-with-data-bricks-for-data-engineering-main/data/retail-data/all/online-retail-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7cef875-4090-4896-9fbc-603ce793f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15492c9c-1ee5-4284-af73-b297930daf26",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02756ff-b819-42ea-814e-2f7154a6a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15174ca9-ee27-4975-afd7-e725d695b95b",
   "metadata": {},
   "source": [
    "## countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56b229ef-196d-4283-9681-ac31b25ebf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c50203-07d3-46cb-9c63-91d0948f9745",
   "metadata": {},
   "source": [
    "## approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc2fb9e-39bb-479f-b38f-b915b2e68e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|approxCountDistinct|\n",
      "+-------------------+\n",
      "|               3364|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.1).alias(\"approxCountDistinct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95f5fd-f7e4-4b3c-9a30-816eb2cb9b7a",
   "metadata": {},
   "source": [
    "## first and last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1294ce53-815e-46fc-838f-0e7a953d4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ec518-dd34-4932-b818-060b4f5498b5",
   "metadata": {},
   "source": [
    "## min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09cc334-aa39-4296-84fd-446fe503ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e1ca3-2a3e-4cb8-8727-f7e6427d786f",
   "metadata": {},
   "source": [
    "## sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31e29912-d646-406b-b756-b1dd611bb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbdbec-895f-4b88-a836-0b47a2e00625",
   "metadata": {},
   "source": [
    "## sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22cdea0d-1770-426a-8f6e-a8ae63c4188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yogeshpoola/.pyenv/versions/3.11.5/envs/devenv/lib/python3.11/site-packages/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n",
      "[Stage 38:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018ce4d-f58a-4585-b537-5af03fe744a3",
   "metadata": {},
   "source": [
    "## avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac30e5f-fc9a-4c7a-a37c-542125958a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   avg(Quantity)|\n",
      "+----------------+\n",
      "|9.55224954743324|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.select(avg(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0150cad-6d7f-42d8-ad3d-67097fe7ecd6",
   "metadata": {},
   "source": [
    "## Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b6fe5cc-7c88-40ff-bc1a-61bc576503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|var_samp(Quantity)| stddev(Quantity)|\n",
      "+------------------+-----------------+\n",
      "|47559.391409298856|218.0811578502344|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance, stddev\n",
    "df.select(variance(\"Quantity\"), stddev(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5be13-21e2-4ce4-80a3-2495c7d4b9c6",
   "metadata": {},
   "source": [
    "## Grouping with Expressions\n",
    "**We do this grouping in two phases. First we specify the column(s) on which we would like to\n",
    "group, and then we specify the aggregation(s).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f98459d-107a-4fa8-bd5d-c0632bea2004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|InvoiceNo|quan|\n",
      "+---------+----+\n",
      "|   536596|   6|\n",
      "|   536938|  14|\n",
      "|   537252|   1|\n",
      "|   537691|  20|\n",
      "|   538041|   1|\n",
      "|   538184|  26|\n",
      "|   538517|  53|\n",
      "|   538879|  19|\n",
      "|   539275|   6|\n",
      "|   539630|  12|\n",
      "|   540499|  24|\n",
      "|   540540|  22|\n",
      "|  C540850|   1|\n",
      "|   540976|  48|\n",
      "|   541432|   4|\n",
      "|   541518| 101|\n",
      "|   541783|  35|\n",
      "|   542026|   9|\n",
      "|   542375|   6|\n",
      "|   536597|  28|\n",
      "+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61769eb2-44ac-4170-ace5-fb3e428b6204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
